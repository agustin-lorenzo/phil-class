{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e1e1a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from sklearn import preprocessing\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30fdd1d",
   "metadata": {},
   "source": [
    "Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfbae09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Absurd Reasoning Absurdity and Suicide Ther...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passion of living) there are probably but two ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that very day addressed him indifferently. He ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this relationship between the absurd and suici...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peregrinos who is born of legend, m and Jules ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>of repugnance which characterizes the sentimen...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>greater, and none wound more, than when that o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>only source of the exalted rank, among human o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>interest, in which that of every individual is...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>as to overrule any one of the general maxims o...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>782 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  labels\n",
       "0    An Absurd Reasoning Absurdity and Suicide Ther...       0\n",
       "1    passion of living) there are probably but two ...       0\n",
       "2    that very day addressed him indifferently. He ...       0\n",
       "3    this relationship between the absurd and suici...       0\n",
       "4    Peregrinos who is born of legend, m and Jules ...       0\n",
       "..                                                 ...     ...\n",
       "777  of repugnance which characterizes the sentimen...       4\n",
       "778  greater, and none wound more, than when that o...       4\n",
       "779  only source of the exalted rank, among human o...       4\n",
       "780  interest, in which that of every individual is...       4\n",
       "781  as to overrule any one of the general maxims o...       4\n",
       "\n",
       "[782 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/data.csv\", index_col=0)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "df.rename(columns={'entry': 'text', 'label': 'labels'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f15349",
   "metadata": {},
   "source": [
    "Creating train/test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ac5ef",
   "metadata": {},
   "source": [
    "Loading the model and tokenizer, checking input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5273b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 1768.73it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForSequenceClassification LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "classifier.weight       | MISSING    | \n",
      "classifier.bias         | MISSING    | \n",
      "pre_classifier.weight   | MISSING    | \n",
      "pre_classifier.bias     | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input='An Absurd Reasoning Absurdity and Suicide There is but one truly serious philosophical problem, and that is suicide. Judging whether life is or is not worth living amounts to answering the fundamental question of philosophy. All the rest— whether or not the world has three dimensions, whether the mind has nine or twelve categories—comes afterwards. These are games; one must first answer. And if it is true, as Nietzsche claims, that a philosopher, to deserve our respect, must preach by example, you can appreciate the importance of that reply, for it will precede the definitive act. These are facts the heart can feel; yet they call for careful study before they become clear to the intellect. If I ask myself how to judge that this question is more urgent than that, I reply that one judges by the actions it entails. I have never seen anyone die for the ontologi-cal argument. Galileo, who held a scientific truth of great importance, abjured it with the greatest ease as soon as it endangered his life. In a certain sense, he did rightThat truth was not worth the stake. Whether the earth or the sun revolves around the other is a matter of profound indifference. To tell the truth, it is a futile question. On the other hand, I see many people die because they judge that life is not worth living. I see others paradoxically getting killed for the ideas or illusions that give them a reason for living (what is called a reason for living is also an excellent reason for dying). I therefore conclude that the meaning of life is the most urgent of questions. How to answer it? On all essential problems (I mean thereby those that run the risk of leading to death or those that intensify the'\n",
      "\n",
      "encoded_input={'input_ids': tensor([[  101,  2019, 18691, 13384, 18691,  3012,  1998,  5920,  2045,  2003,\n",
      "          2021,  2028,  5621,  3809,  9569,  3291,  1010,  1998,  2008,  2003,\n",
      "          5920,  1012, 13325,  3251,  2166,  2003,  2030,  2003,  2025,  4276,\n",
      "          2542,  8310,  2000, 10739,  1996,  8050,  3160,  1997,  4695,  1012,\n",
      "          2035,  1996,  2717,  1517,  3251,  2030,  2025,  1996,  2088,  2038,\n",
      "          2093,  9646,  1010,  3251,  1996,  2568,  2038,  3157,  2030,  4376,\n",
      "          7236,  1517,  3310,  5728,  1012,  2122,  2024,  2399,  1025,  2028,\n",
      "          2442,  2034,  3437,  1012,  1998,  2065,  2009,  2003,  2995,  1010,\n",
      "          2004, 28898,  4447,  1010,  2008,  1037,  9667,  1010,  2000, 10107,\n",
      "          2256,  4847,  1010,  2442, 25250,  2011,  2742,  1010,  2017,  2064,\n",
      "          9120,  1996,  5197,  1997,  2008,  7514,  1010,  2005,  2009,  2097,\n",
      "          3653, 22119,  1996, 15764,  2552,  1012,  2122,  2024,  8866,  1996,\n",
      "          2540,  2064,  2514,  1025,  2664,  2027,  2655,  2005,  6176,  2817,\n",
      "          2077,  2027,  2468,  3154,  2000,  1996, 24823,  1012,  2065,  1045,\n",
      "          3198,  2870,  2129,  2000,  3648,  2008,  2023,  3160,  2003,  2062,\n",
      "         13661,  2084,  2008,  1010,  1045,  7514,  2008,  2028,  6794,  2011,\n",
      "          1996,  4506,  2009,  4372, 22081,  1012,  1045,  2031,  2196,  2464,\n",
      "          3087,  3280,  2005,  1996,  3031, 21197,  2072,  1011, 10250,  6685,\n",
      "          1012, 21514,  1010,  2040,  2218,  1037,  4045,  3606,  1997,  2307,\n",
      "          5197,  1010, 11113, 26949,  2009,  2007,  1996,  4602,  7496,  2004,\n",
      "          2574,  2004,  2009, 10193,  2010,  2166,  1012,  1999,  1037,  3056,\n",
      "          3168,  1010,  2002,  2106,  2157,  8322,  2102,  3606,  2001,  2025,\n",
      "          4276,  1996,  8406,  1012,  3251,  1996,  3011,  2030,  1996,  3103,\n",
      "         19223,  2105,  1996,  2060,  2003,  1037,  3043,  1997, 13769, 25920,\n",
      "          1012,  2000,  2425,  1996,  3606,  1010,  2009,  2003,  1037, 24495,\n",
      "          3160,  1012,  2006,  1996,  2060,  2192,  1010,  1045,  2156,  2116,\n",
      "          2111,  3280,  2138,  2027,  3648,  2008,  2166,  2003,  2025,  4276,\n",
      "          2542,  1012,  1045,  2156,  2500, 20506, 15004,  2893,  2730,  2005,\n",
      "          1996,  4784,  2030, 24883,  2008,  2507,  2068,  1037,  3114,  2005,\n",
      "          2542,  1006,  2054,  2003,  2170,  1037,  3114,  2005,  2542,  2003,\n",
      "          2036,  2019,  6581,  3114,  2005,  5996,  1007,  1012,  1045,  3568,\n",
      "         16519,  2008,  1996,  3574,  1997,  2166,  2003,  1996,  2087, 13661,\n",
      "          1997,  3980,  1012,  2129,  2000,  3437,  2009,  1029,  2006,  2035,\n",
      "          6827,  3471,  1006,  1045,  2812,  8558,  2216,  2008,  2448,  1996,\n",
      "          3891,  1997,  2877,  2000,  2331,  2030,  2216,  2008, 20014,  6132,\n",
      "          8757,  1996,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "output=SequenceClassifierOutput(loss=None, logits=tensor([[-0.0604, -0.0169]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "input = df['text'][0]\n",
    "encoded_input = tokenizer(input, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(f\"\\n{input=}\")\n",
    "print(f\"\\n{encoded_input=}\")\n",
    "print(f\"\\n{output=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa0e42d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 625/625 [00:00<00:00, 5928.94 examples/s]\n",
      "Map: 100%|██████████| 157/157 [00:00<00:00, 5910.02 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 625\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 157\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train, X_text, y_train, y_text = train_test_split(df['entry'], df['label'], test_size=0.2, random_state=42)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "218e9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec52f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agustinlorenzjo/Developer/phil-class/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    eval_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
